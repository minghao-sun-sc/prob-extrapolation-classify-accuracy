import numpy as np
import matplotlib.pyplot as plt
import torch
import gpytorch
import sys
import os
import glob

# Add the src directory to the Python path
sys.path.append(os.path.abspath('../src'))
# Add src to module import path
sys.path.insert(0, os.path.abspath('../src'))

# Now import our modules
import models
import beta_priors
import matern_kernels
import combo_kernels

# Set torch float precision
torch.set_default_dtype(torch.float64)

def load_encoded_data():
    """Load encoded dataset or use placeholder if not available"""
    # Path to encoded images
    encoded_path = '../dataset/encoded_images/'
    
    # List available encoded data files
    encoded_files = glob.glob(os.path.join(encoded_path, '*.npz'))
    print(f"Available encoded datasets: {encoded_files}")
    
    # If we have encoded datasets, load one of them
    if encoded_files:
        # Load the first available dataset
        data = np.load(encoded_files[0])
        print(f"Loaded dataset: {encoded_files[0]}")
        print(f"Available keys: {data.files}")
        
        try:
            if 'train_sizes' in data.files and 'train_scores' in data.files:
                X = torch.tensor(data['train_sizes'], dtype=torch.float64)
                y = torch.tensor(data['train_scores'], dtype=torch.float64)
                print(f"Data loaded successfully: {len(X)} data points")
                return X, y
            else:
                # Look for any arrays that could represent training sizes and accuracies
                print("Standard keys not found, examining data structure...")
                for key in data.files:
                    print(f"{key}: shape {data[key].shape}, type {data[key].dtype}")
                
                # Try to find appropriate data
                potential_x = None
                potential_y = None
                
                for key in data.files:
                    arr = data[key]
                    # Look for arrays that could be training sizes or accuracies
                    if len(arr.shape) == 1:
                        if potential_x is None and arr.min() > 10:  # Likely to be training sizes
                            potential_x = arr
                        elif potential_y is None and 0 <= arr.min() <= arr.max() <= 1:  # Likely to be accuracies
                            potential_y = arr
                
                if potential_x is not None and potential_y is not None:
                    print("Found potential learning curve data")
                    X = torch.tensor(potential_x, dtype=torch.float64)
                    y = torch.tensor(potential_y, dtype=torch.float64)
                    return X, y
                    
        except Exception as e:
            print(f"Error loading data: {e}")
    
    # Fallback to placeholder data
    print("Using placeholder data")
    X = torch.tensor([64, 100, 200, 300, 400, 500], dtype=torch.float64)
    y = torch.tensor([0.60, 0.62, 0.64, 0.66, 0.67, 0.67], dtype=torch.float64)
    return X, y

def plot_learning_curve(X, y):
    """Plot the learning curve data"""
    plt.figure(figsize=(10, 6))
    plt.scatter(X.numpy(), y.numpy(), c='k')
    plt.xscale('log')
    plt.xlabel('Training set size')
    plt.ylabel('Accuracy')
    plt.title('Learning Curve Data')
    plt.grid(True, which="both", ls="-")
    plt.show()

def experiment_beta_priors(X, y, X_test):
    """Experiment with different beta prior configurations"""
    print("\n=== Experiment: Beta Priors ===")
    
    beta_configs = [
        {'alpha': 1, 'beta': 3, 'name': 'Beta(1,3) - Right skewed'},
        {'alpha': 2, 'beta': 2, 'name': 'Beta(2,2) - Symmetric'},
        {'alpha': 5, 'beta': 2, 'name': 'Beta(5,2) - Left skewed'},
        {'alpha': 10, 'beta': 2, 'name': 'Beta(10,2) - Strongly left skewed'}
    ]
    
    models_beta = []
    likelihoods_beta = []
    
    for config in beta_configs:
        print(f"Training with {config['name']}")
        likelihood, model, losses = beta_priors.train_gp_with_beta_prior(
            X, y, 
            alpha=config['alpha'], 
            beta=config['beta'],
            epsilon_min=0.05,
            lr=0.01,
            training_iter=20000
        )
        models_beta.append((model, config['name']))
        likelihoods_beta.append(likelihood)
        
        # Plot loss curve
        plt.figure(figsize=(10, 5))
        plt.plot(losses)
        plt.yscale('log')
        plt.title(f"Training Loss for {config['name']}")
        plt.xlabel('Iterations')
        plt.ylabel('Loss (log scale)')
        plt.grid(True, which="both", ls="-")
        plt.show()
    
    # Plot predictions
    plt.figure(figsize=(12, 8))
    plt.scatter(X.numpy(), y.numpy(), c='k', label='Training data')
    
    colors = ['blue', 'red', 'green', 'purple']
    
    for i, (model, name) in enumerate(models_beta):
        model.eval()
        likelihoods_beta[i].eval()
        with torch.no_grad(), gpytorch.settings.fast_pred_var():
            predictions = likelihoods_beta[i](model(X_test))
            mean = predictions.mean
            lower, upper = predictions.confidence_region()
        
        plt.plot(X_test.numpy(), mean.numpy(), colors[i], label=name)
        plt.fill_between(X_test.numpy(), lower.numpy(), upper.numpy(), alpha=0.2, color=colors[i])
    
    plt.xscale('log')
    plt.xlabel('Training set size')
    plt.ylabel('Accuracy')
    plt.title('Comparing Different Beta Prior Configurations for Learning Curve Extrapolation')
    plt.legend()
    plt.grid(True, which="both", ls="-")
    plt.show()
    
    return models_beta, likelihoods_beta

def experiment_composite_kernels(X, y, X_test):
    """Experiment with different composite kernel configurations"""
    print("\n=== Experiment: Composite Kernels ===")
    
    kernel_configs = [
        {'name': 'Composite RBF+Periodic+Matern', 'type': 'composite', 'periodic_period': 2.0, 'matern_nu': 1.5},
        {'name': 'Spectral Mixture (4 mixtures)', 'type': 'spectral', 'num_mixtures': 4},
        {'name': 'Deep Kernel (MLP)', 'type': 'deep', 'hidden_dims': [32, 16]}
    ]
    
    models_kernel = []
    likelihoods_kernel = []
    
    for config in kernel_configs:
        print(f"Training with {config['name']}")
        
        if config['type'] == 'composite':
            likelihood, model, losses = combo_kernels.train_composite_kernel(
                X, y, 
                epsilon_min=0.05,
                periodic_period=config['periodic_period'],
                matern_nu=config['matern_nu'],
                lr=0.01,
                training_iter=20000
            )
        elif config['type'] == 'spectral':
            likelihood, model, losses = combo_kernels.train_spectral_mixture(
                X, y, 
                epsilon_min=0.05,
                num_mixtures=config['num_mixtures'],
                lr=0.01,
                training_iter=20000
            )
        elif config['type'] == 'deep':
            likelihood, model, losses = combo_kernels.train_deep_kernel(
                X, y, 
                epsilon_min=0.05,
                hidden_dims=config['hidden_dims'],
                lr=0.01,
                training_iter=20000
            )
        
        models_kernel.append((model, config['name']))
        likelihoods_kernel.append(likelihood)
        
        # Plot loss curve
        plt.figure(figsize=(10, 5))
        plt.plot(losses)
        plt.yscale('log')
        plt.title(f"Training Loss for {config['name']}")
        plt.xlabel('Iterations')
        plt.ylabel('Loss (log scale)')
        plt.grid(True, which="both", ls="-")
        plt.show()
    
    # Plot predictions
    plt.figure(figsize=(12, 8))
    plt.scatter(X.numpy(), y.numpy(), c='k', label='Training data')
    
    colors = ['blue', 'red', 'green', 'purple']
    
    for i, (model, name) in enumerate(models_kernel):
        model.eval()
        likelihoods_kernel[i].eval()
        with torch.no_grad(), gpytorch.settings.fast_pred_var():
            predictions = likelihoods_kernel[i](model(X_test))
            mean = predictions.mean
            lower, upper = predictions.confidence_region()
        
        plt.plot(X_test.numpy(), mean.numpy(), colors[i], label=name)
        plt.fill_between(X_test.numpy(), lower.numpy(), upper.numpy(), alpha=0.2, color=colors[i])
    
    plt.xscale('log')
    plt.xlabel('Training set size')
    plt.ylabel('Accuracy')
    plt.title('Comparing Different Kernel Configurations for Learning Curve Extrapolation')
    plt.legend()
    plt.grid(True, which="both", ls="-")
    plt.show()
    
    return models_kernel, likelihoods_kernel

def experiment_model_ensemble(X, y, X_test, models_beta, likelihoods_beta, models_kernel, likelihoods_kernel):
    """Combine predictions from multiple models"""
    print("\n=== Experiment: Model Ensemble ===")
    
    plt.figure(figsize=(12, 8))
    plt.scatter(X.numpy(), y.numpy(), c='k', label='Training data')
    
    all_means = []
    all_variances = []
    
    # Baseline model
    print("Training baseline RBF model")
    likelihood_rbf = gpytorch.likelihoods.GaussianLikelihood()
    model_rbf = models.GPPowerLaw(X, y, likelihood_rbf, epsilon_min=0.05, with_priors=True)
    likelihood_rbf, model_rbf, _ = models.train_gp(likelihood_rbf, model_rbf, X, y, max_iters=20000, lr=0.01)
    
    model_rbf.eval()
    likelihood_rbf.eval()
    with torch.no_grad(), gpytorch.settings.fast_pred_var():
        predictions_rbf = likelihood_rbf(model_rbf(X_test))
        mean_rbf = predictions_rbf.mean.numpy()
        lower_rbf, upper_rbf = predictions_rbf.confidence_region()
        variance_rbf = ((upper_rbf - lower_rbf).numpy() / 4) ** 2
    
    plt.plot(X_test.numpy(), mean_rbf, 'b-', label='Baseline RBF kernel')
    plt.fill_between(X_test.numpy(), lower_rbf.numpy(), upper_rbf.numpy(), alpha=0.2, color='blue')
    
    all_means.append(mean_rbf)
    all_variances.append(variance_rbf)
    
    # Add beta prior models
    for i, (model, name) in enumerate(models_beta):
        model.eval()
        likelihoods_beta[i].eval()
        with torch.no_grad(), gpytorch.settings.fast_pred_var():
            predictions = likelihoods_beta[i](model(X_test))
            mean = predictions.mean.numpy()
            lower, upper = predictions.confidence_region()
            variance = ((upper - lower).numpy() / 4) ** 2
        
        all_means.append(mean)
        all_variances.append(variance)
    
    # Add kernel models
    for i, (model, name) in enumerate(models_kernel):
        model.eval()
        likelihoods_kernel[i].eval()
        with torch.no_grad(), gpytorch.settings.fast_pred_var():
            predictions = likelihoods_kernel[i](model(X_test))
            mean = predictions.mean.numpy()
            lower, upper = predictions.confidence_region()
            variance = ((upper - lower).numpy() / 4) ** 2
        
        all_means.append(mean)
        all_variances.append(variance)
    
    # Combine models - simple average
    combined_mean_simple = np.mean(all_means, axis=0)
    
    # Combine models - weighted by inverse variance
    weights = 1.0 / np.array(all_variances)
    weights = weights / np.sum(weights, axis=0, keepdims=True)
    combined_mean_weighted = np.sum(weights * np.array(all_means), axis=0)
    
    # Combined variance
    combined_variance = np.mean(all_variances, axis=0) / len(all_variances)
    combined_std = np.sqrt(combined_variance)
    
    # Plot individuals faded
    for mean in all_means:
        plt.plot(X_test.numpy(), mean, alpha=0.3)
    
    # Plot combined
    plt.plot(X_test.numpy(), combined_mean_simple, 'g-', linewidth=2, label='Ensemble (simple average)')
    plt.plot(X_test.numpy(), combined_mean_weighted, 'm-', linewidth=2, label='Ensemble (variance-weighted)')
    
    # Plot confidence bands
    plt.fill_between(
        X_test.numpy(),
        combined_mean_weighted - 2*combined_std,
        combined_mean_weighted + 2*combined_std,
        alpha=0.2, color='m'
    )
    
    plt.xscale('log')
    plt.xlabel('Training set size')
    plt.ylabel('Accuracy')
    plt.title('Model Ensemble for Learning Curve Extrapolation')
    plt.legend()
    plt.grid(True, which="both", ls="-")
    plt.ylim(0.5, 1.0)
    plt.show()

if __name__ == "__main__":
    # Load the data
    X, y = load_encoded_data()
    
    # Plot the learning curve data
    plot_learning_curve(X, y)
    
    # Define extrapolation range
    X_test = torch.logspace(np.log10(X.min()), np.log10(50000), 100)
    
    # Run experiments
    models_beta, likelihoods_beta = experiment_beta_priors(X, y, X_test)
    models_kernel, likelihoods_kernel = experiment_composite_kernels(X, y, X_test)
    experiment_model_ensemble(X, y, X_test, models_beta, likelihoods_beta, models_kernel, likelihoods_kernel)
    
    print("All experiments completed!")