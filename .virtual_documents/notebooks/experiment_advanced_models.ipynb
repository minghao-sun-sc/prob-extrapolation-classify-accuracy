


import numpy as np
import matplotlib.pyplot as plt
import torch
import gpytorch
import sys
sys.path.append('../')
from src import models, beta_priors, matern_kernels, combo_kernels

# Set torch float precision
torch.set_default_dtype(torch.float64)





# Sample dataset (replace with your actual data)
# X represents training set sizes, y represents accuracy values
X = torch.tensor([64, 100, 200, 300, 400, 500])
y = torch.tensor([0.60, 0.62, 0.64, 0.66, 0.67, 0.67])

# Define extrapolation range
X_test = torch.logspace(np.log10(X.min()), np.log10(50000), 100)





# Train different beta prior configurations
beta_configs = [
    {'alpha': 1, 'beta': 3, 'name': 'Beta(1,3) - Right skewed'},
    {'alpha': 2, 'beta': 2, 'name': 'Beta(2,2) - Symmetric'},
    {'alpha': 5, 'beta': 2, 'name': 'Beta(5,2) - Left skewed'},
    {'alpha': 10, 'beta': 2, 'name': 'Beta(10,2) - Strongly left skewed'}
]

models_beta = []
likelihoods_beta = []

for config in beta_configs:
    print(f"Training with {config['name']}")
    likelihood, model, losses = beta_priors.train_gp_with_beta_prior(
        X, y, 
        alpha=config['alpha'], 
        beta=config['beta'],
        epsilon_min=0.05,
        lr=0.01,
        training_iter=30000
    )
    models_beta.append((model, config['name']))
    likelihoods_beta.append(likelihood)
    
    # Plot loss curve
    plt.figure(figsize=(10, 5))
    plt.plot(losses)
    plt.yscale('log')
    plt.title(f'Training Loss for {config["name"]}')
    plt.xlabel('Iterations')
    plt.ylabel('Loss (log scale)')
    plt.grid(True, which="both", ls="-")
    plt.show()


# Evaluate and plot the results for beta prior models
plt.figure(figsize=(12, 8))

# Plot the training data
plt.scatter(X.numpy(), y.numpy(), c='k', label='Training data')

colors = ['blue', 'red', 'green', 'purple']

# Plot predictions for each model
for i, (model, name) in enumerate(models_beta):
    # Get predictions
    model.eval()
    likelihoods_beta[i].eval()
    with torch.no_grad(), gpytorch.settings.fast_pred_var():
        predictions = likelihoods_beta[i](model(X_test))
        mean = predictions.mean
        lower, upper = predictions.confidence_region()
    
    # Plot the mean prediction
    plt.plot(X_test.numpy(), mean.numpy(), colors[i], label=name)
    
    # Plot the confidence region
    plt.fill_between(X_test.numpy(), lower.numpy(), upper.numpy(), alpha=0.2, color=colors[i])

plt.xscale('log')
plt.xlabel('Training set size')
plt.ylabel('Accuracy')
plt.title('Comparing Different Beta Prior Configurations for Learning Curve Extrapolation')
plt.legend()
plt.grid(True, which="both", ls="-")
plt.show()





# Train different composite kernel configurations
kernel_configs = [
    {'name': 'Composite RBF+Periodic+Matern', 'periodic_period': 2.0, 'matern_nu': 1.5},
    {'name': 'Spectral Mixture (4 mixtures)', 'num_mixtures': 4},
    {'name': 'Deep Kernel (MLP)', 'hidden_dims': [32, 16]},
    {'name': 'Deep Kernel (larger)', 'hidden_dims': [64, 32, 16]}
]

models_kernel = []
likelihoods_kernel = []

for config in kernel_configs:
    print(f"Training with {config['name']}")
    
    if 'Composite' in config['name']:
        likelihood, model, losses = combo_kernels.train_composite_kernel(
            X, y, 
            epsilon_min=0.05,
            periodic_period=config['periodic_period'],
            matern_nu=config['matern_nu'],
            lr=0.01,
            training_iter=30000
        )
    elif 'Spectral' in config['name']:
        likelihood, model, losses = combo_kernels.train_spectral_mixture(
            X, y, 
            epsilon_min=0.05,
            num_mixtures=config['num_mixtures'],
            lr=0.01,
            training_iter=30000
        )
    elif 'Deep Kernel' in config['name']:
        likelihood, model, losses = combo_kernels.train_deep_kernel(
            X, y, 
            epsilon_min=0.05,
            hidden_dims=config['hidden_dims'],
            lr=0.01,
            training_iter=30000
        )
        
    models_kernel.append((model, config['name']))
    likelihoods_kernel.append(likelihood)
    
    # Plot loss curve
    plt.figure(figsize=(10, 5))
    plt.plot(losses)
    plt.yscale('log')
    plt.title(f'Training Loss for {config["name"]}')
    plt.xlabel('Iterations')
    plt.ylabel('Loss (log scale)')
    plt.grid(True, which="both", ls="-")
    plt.show()


# Evaluate and plot the results for composite kernel models
plt.figure(figsize=(12, 8))

# Plot the training data
plt.scatter(X.numpy(), y.numpy(), c='k', label='Training data')

colors = ['blue', 'red', 'green', 'purple']

# Plot predictions for each model
for i, (model, name) in enumerate(models_kernel):
    # Get predictions
    model.eval()
    likelihoods_kernel[i].eval()
    with torch.no_grad(), gpytorch.settings.fast_pred_var():
        predictions = likelihoods_kernel[i](model(X_test))
        mean = predictions.mean
        lower, upper = predictions.confidence_region()
    
    # Plot the mean prediction
    plt.plot(X_test.numpy(), mean.numpy(), colors[i], label=name)
    
    # Plot the confidence region
    plt.fill_between(X_test.numpy(), lower.numpy(), upper.numpy(), alpha=0.2, color=colors[i])

plt.xscale('log')
plt.xlabel('Training set size')
plt.ylabel('Accuracy')
plt.title('Comparing Different Kernel Configurations for Learning Curve Extrapolation')
plt.legend()
plt.grid(True, which="both", ls="-")
plt.show()





# Grid search over hyperparameters
alpha_values = [1, 2, 5, 10]
beta_values = [1, 2, 3, 5]
epsilon_min_values = [0.01, 0.05, 0.1]
output_scale_mean_values = [0.5, 1.0, 2.0]

best_model = None
best_likelihood = None
best_loss = float('inf')
best_config = {}

# We'll use a smaller number of iterations for the grid search
grid_search_iters = 5000

results = []

# Run a subset of parameter combinations to save time
for alpha in alpha_values:
    for beta in beta_values:
        for epsilon_min in [0.05]:  # Just try one value for grid search
            for output_scale_mean in [1.0]:  # Just try one value for grid search
                print(f"Training model with alpha={alpha}, beta={beta}, epsilon_min={epsilon_min}")
                
                likelihood, model, losses = beta_priors.train_gp_with_beta_prior(
                    X, y, 
                    alpha=alpha, 
                    beta=beta,
                    epsilon_min=epsilon_min,
                    output_scale_prior_mean=output_scale_mean,
                    lr=0.01,
                    training_iter=grid_search_iters
                )
                
                final_loss = losses[-1]
                results.append({
                    'alpha': alpha,
                    'beta': beta,
                    'epsilon_min': epsilon_min,
                    'output_scale_mean': output_scale_mean,
                    'final_loss': final_loss
                })
                
                if final_loss < best_loss:
                    best_loss = final_loss
                    best_model = model
                    best_likelihood = likelihood
                    best_config = {
                        'alpha': alpha,
                        'beta': beta,
                        'epsilon_min': epsilon_min,
                        'output_scale_mean': output_scale_mean
                    }


# Display results sorted by final loss
import pandas as pd

df_results = pd.DataFrame(results)
df_results_sorted = df_results.sort_values('final_loss')
df_results_sorted.head(10)


# Train the best model for more iterations
print(f"Training best model with config: {best_config}")

likelihood_best, model_best, losses_best = beta_priors.train_gp_with_beta_prior(
    X, y, 
    alpha=best_config['alpha'], 
    beta=best_config['beta'],
    epsilon_min=best_config['epsilon_min'],
    output_scale_prior_mean=best_config['output_scale_mean'],
    lr=0.01,
    training_iter=50000
)

# Plot loss curve
plt.figure(figsize=(10, 5))
plt.plot(losses_best)
plt.yscale('log')
plt.title(f'Training Loss for Best Model')
plt.xlabel('Iterations')
plt.ylabel('Loss (log scale)')
plt.grid(True, which="both", ls="-")
plt.show()


# Compare best model with baseline
plt.figure(figsize=(12, 8))

# Plot the training data
plt.scatter(X.numpy(), y.numpy(), c='k', label='Training data')

# Train standard RBF model as baseline
likelihood_rbf = gpytorch.likelihoods.GaussianLikelihood()
model_rbf = models.GPPowerLaw(X, y, likelihood_rbf, epsilon_min=0.05, with_priors=True)
likelihood_rbf, model_rbf, losses_rbf = models.train_gp(likelihood_rbf, model_rbf, X, y, max_iters=30000, lr=0.01)

# Plot baseline model prediction
model_rbf.eval()
likelihood_rbf.eval()
with torch.no_grad(), gpytorch.settings.fast_pred_var():
    predictions_rbf = likelihood_rbf(model_rbf(X_test))
    mean_rbf = predictions_rbf.mean
    lower_rbf, upper_rbf = predictions_rbf.confidence_region()

plt.plot(X_test.numpy(), mean_rbf.numpy(), 'b-', label='Baseline RBF kernel')
plt.fill_between(X_test.numpy(), lower_rbf.numpy(), upper_rbf.numpy(), alpha=0.2, color='blue')

# Plot best model prediction
model_best.eval()
likelihood_best.eval()
with torch.no_grad(), gpytorch.settings.fast_pred_var():
    predictions_best = likelihood_best(model_best(X_test))
    mean_best = predictions_best.mean
    lower_best, upper_best = predictions_best.confidence_region()

plt.plot(X_test.numpy(), mean_best.numpy(), 'r-', label=f'Best model: Beta({best_config["alpha"]},{best_config["beta"]})')
plt.fill_between(X_test.numpy(), lower_best.numpy(), upper_best.numpy(), alpha=0.2, color='red')

plt.xscale('log')
plt.xlabel('Training set size')
plt.ylabel('Accuracy')
plt.title('Comparing Best Model with Baseline')
plt.legend()
plt.grid(True, which="both", ls="-")
plt.show()





# Combine predictions from different models
plt.figure(figsize=(12, 8))

# Plot the training data
plt.scatter(X.numpy(), y.numpy(), c='k', label='Training data')

# Collect predictions from all models
all_means = []
all_variances = []

# Add kernel models
for i, (model, name) in enumerate(models_kernel):
    model.eval()
    likelihoods_kernel[i].eval()
    with torch.no_grad(), gpytorch.settings.fast_pred_var():
        predictions = likelihoods_kernel[i](model(X_test))
        mean = predictions.mean.numpy()
        # Extract variance from confidence region
        lower, upper = predictions.confidence_region()
        variance = ((upper - lower).numpy() / 4) ** 2  # Approximate variance from 95% CI
        
    all_means.append(mean)
    all_variances.append(variance)

# Add the best beta prior model
model_best.eval()
likelihood_best.eval()
with torch.no_grad(), gpytorch.settings.fast_pred_var():
    predictions_best = likelihood_best(model_best(X_test))
    mean_best = predictions_best.mean.numpy()
    lower_best, upper_best = predictions_best.confidence_region()
    variance_best = ((upper_best - lower_best).numpy() / 4) ** 2

all_means.append(mean_best)
all_variances.append(variance_best)

# Combine models - simple average
combined_mean_simple = np.mean(all_means, axis=0)

# Combine models - weighted by inverse variance (more confident models get higher weight)
weights = 1.0 / np.array(all_variances)
weights = weights / np.sum(weights, axis=0, keepdims=True)  # Normalize
combined_mean_weighted = np.sum(weights * np.array(all_means), axis=0)

# Combined variance (approximation)
combined_variance = np.mean(all_variances, axis=0) / len(all_variances)  # Reduced by ensemble size
combined_std = np.sqrt(combined_variance)

# Plot individual models (faded)
for i, mean in enumerate(all_means):
    plt.plot(X_test.numpy(), mean, alpha=0.3)

# Plot combined predictions
plt.plot(X_test.numpy(), combined_mean_simple, 'g-', linewidth=2, label='Ensemble (simple average)')
plt.plot(X_test.numpy(), combined_mean_weighted, 'm-', linewidth=2, label='Ensemble (variance-weighted)')

# Plot confidence bands for weighted ensemble
plt.fill_between(
    X_test.numpy(), 
    combined_mean_weighted - 2*combined_std, 
    combined_mean_weighted + 2*combined_std, 
    alpha=0.2, color='m'
)

plt.xscale('log')
plt.xlabel('Training set size')
plt.ylabel('Accuracy')
plt.title('Model Ensemble for Learning Curve Extrapolation')
plt.legend()
plt.grid(True, which="both", ls="-")
plt.ylim(0.5, 1.0)  # Set y-axis limits between 0.5 and 1.0 (reasonable for accuracy)
plt.show()
